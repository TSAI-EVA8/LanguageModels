{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "from transformers import AutoTokenizer  # pip install transformers\n",
    "from utils import (\n",
    "    BATCH_SIZE,\n",
    "    BLOCK_SIZE,\n",
    "    DEVICE,\n",
    "    DROPOUT,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EMBED,\n",
    "    NUM_HEAD,\n",
    "    NUM_LAYER,\n",
    "    MAX_ITER,\n",
    "    EVAL_INTER,\n",
    "    encode,\n",
    "    decode,\n",
    "    get_batch,\n",
    "    save_model_to_chekpoint,\n",
    "    estimate_loss,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the data and split it into the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (166628 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# raw data\n",
    "path_do_data = \"data/wikiPages_football.txt\"\n",
    "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
    "# we use pretrained BERT tokenizer for performance improvements\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# data_raw = data_raw[4000000:] # short dataset\n",
    "\n",
    "# train/val split\n",
    "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 89.48M parameters\n"
     ]
    }
   ],
   "source": [
    "# train a new model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=NUM_EMBED,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEAD,\n",
    "    num_layers=NUM_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "# load model to GPU if available\n",
    "m = model.to(DEVICE)\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step          0 | train loss 10.7338 | val loss 10.7233\n",
      "step         50 | train loss 6.2606 | val loss 6.7147\n",
      "step        100 | train loss 5.7564 | val loss 6.3920\n",
      "step        150 | train loss 5.4606 | val loss 6.3821\n",
      "step        200 | train loss 5.2024 | val loss 6.3680\n",
      "step        250 | train loss 4.8229 | val loss 6.3141\n",
      "step        300 | train loss 4.6281 | val loss 6.3638\n",
      "step        350 | train loss 4.4722 | val loss 6.2245\n",
      "step        400 | train loss 4.4075 | val loss 6.2456\n",
      "step        450 | train loss 4.2228 | val loss 6.2939\n",
      "step        500 | train loss 4.1904 | val loss 6.2985\n",
      "step        550 | train loss 4.1205 | val loss 6.2752\n",
      "step        600 | train loss 4.1011 | val loss 6.3474\n",
      "step        650 | train loss 4.0649 | val loss 6.3941\n",
      "step        700 | train loss 4.0347 | val loss 6.4135\n",
      "step        750 | train loss 4.0890 | val loss 6.3949\n",
      "step        800 | train loss 4.0450 | val loss 6.2727\n",
      "step        850 | train loss 4.0289 | val loss 6.5259\n",
      "step        900 | train loss 4.1181 | val loss 6.4569\n",
      "step        950 | train loss 4.2835 | val loss 6.6969\n",
      "step       1000 | train loss 4.1880 | val loss 6.4577\n",
      "step       1050 | train loss 4.2164 | val loss 6.5906\n",
      "step       1100 | train loss 4.2546 | val loss 6.5679\n",
      "step       1150 | train loss 4.5527 | val loss 6.5519\n",
      "step       1200 | train loss 4.2659 | val loss 6.6196\n",
      "step       1250 | train loss 4.4469 | val loss 6.6927\n",
      "step       1300 | train loss 4.4382 | val loss 6.7087\n",
      "step       1350 | train loss 4.3811 | val loss 6.6688\n",
      "step       1400 | train loss 4.4149 | val loss 6.6932\n",
      "step       1450 | train loss 4.4066 | val loss 6.6356\n",
      "step       1500 | train loss 4.5141 | val loss 6.7647\n",
      "step       1550 | train loss 4.5190 | val loss 6.7658\n",
      "step       1600 | train loss 4.3490 | val loss 6.6278\n",
      "step       1650 | train loss 4.6759 | val loss 6.8272\n",
      "step       1700 | train loss 4.5962 | val loss 6.8345\n",
      "step       1750 | train loss 4.4014 | val loss 6.7044\n",
      "step       1800 | train loss 4.5371 | val loss 6.7421\n",
      "step       1850 | train loss 4.4785 | val loss 6.7364\n",
      "step       1900 | train loss 4.3915 | val loss 6.6980\n",
      "step       1950 | train loss 4.7781 | val loss 6.9498\n",
      "step       2000 | train loss 4.5176 | val loss 6.7689\n",
      "step       2050 | train loss 4.6321 | val loss 6.6576\n",
      "step       2100 | train loss 4.5163 | val loss 6.7726\n",
      "step       2150 | train loss 4.5252 | val loss 6.7528\n",
      "step       2200 | train loss 4.4667 | val loss 6.7476\n",
      "step       2250 | train loss 4.6472 | val loss 6.8369\n",
      "step       2300 | train loss 4.6549 | val loss 6.7636\n",
      "step       2350 | train loss 4.7510 | val loss 6.9026\n",
      "step       2400 | train loss 4.6805 | val loss 6.9033\n",
      "step       2450 | train loss 4.6483 | val loss 6.7866\n",
      "step       2500 | train loss 4.6881 | val loss 6.6791\n",
      "step       2550 | train loss 4.7144 | val loss 6.6775\n",
      "step       2600 | train loss 4.8677 | val loss 7.0239\n",
      "step       2650 | train loss 4.6953 | val loss 6.8412\n",
      "step       2700 | train loss 4.8167 | val loss 7.0007\n",
      "step       2750 | train loss 4.7742 | val loss 6.9097\n",
      "step       2800 | train loss 4.7769 | val loss 6.8694\n",
      "step       2850 | train loss 4.6724 | val loss 6.7458\n",
      "step       2900 | train loss 4.6150 | val loss 6.8118\n",
      "step       2950 | train loss 4.7610 | val loss 6.9168\n",
      "step       3000 | train loss 5.1311 | val loss 7.1798\n",
      "step       3050 | train loss 4.8844 | val loss 7.0299\n",
      "step       3100 | train loss 4.9911 | val loss 7.1409\n",
      "step       3150 | train loss 4.9481 | val loss 7.0613\n",
      "step       3200 | train loss 5.0338 | val loss 6.9550\n",
      "step       3250 | train loss 4.8485 | val loss 6.9595\n",
      "step       3300 | train loss 4.8851 | val loss 7.0028\n",
      "step       3350 | train loss 4.8065 | val loss 6.8780\n",
      "step       3400 | train loss 4.7573 | val loss 6.8481\n",
      "step       3450 | train loss 4.7517 | val loss 6.9073\n",
      "step       3500 | train loss 5.1698 | val loss 7.0791\n",
      "step       3550 | train loss 4.8325 | val loss 6.8703\n",
      "step       3600 | train loss 4.8193 | val loss 6.9655\n",
      "step       3650 | train loss 4.8681 | val loss 6.9365\n",
      "step       3700 | train loss 5.1034 | val loss 7.0154\n",
      "step       3750 | train loss 5.0159 | val loss 7.0014\n",
      "step       3800 | train loss 4.8875 | val loss 7.0553\n",
      "step       3850 | train loss 4.9422 | val loss 6.9234\n",
      "step       3900 | train loss 4.9467 | val loss 6.9574\n",
      "step       3950 | train loss 4.9355 | val loss 6.8781\n",
      "step       4000 | train loss 4.8606 | val loss 7.0751\n",
      "step       4050 | train loss 4.8428 | val loss 7.0108\n",
      "step       4100 | train loss 4.8983 | val loss 6.9587\n",
      "step       4150 | train loss 4.9879 | val loss 7.0422\n",
      "step       4200 | train loss 5.0032 | val loss 7.0010\n",
      "step       4250 | train loss 5.0209 | val loss 7.0528\n",
      "step       4300 | train loss 5.0604 | val loss 7.0991\n",
      "step       4350 | train loss 5.1225 | val loss 7.1366\n",
      "step       4400 | train loss 5.1237 | val loss 7.0489\n",
      "step       4450 | train loss 5.2680 | val loss 7.1349\n",
      "step       4500 | train loss 5.1351 | val loss 7.1407\n",
      "step       4550 | train loss 5.1196 | val loss 7.0769\n",
      "step       4600 | train loss 5.1260 | val loss 7.0085\n",
      "step       4650 | train loss 5.0779 | val loss 7.0093\n",
      "step       4700 | train loss 5.2175 | val loss 6.9642\n",
      "step       4750 | train loss 5.1181 | val loss 7.1019\n",
      "step       4800 | train loss 5.1230 | val loss 7.1253\n",
      "step       4850 | train loss 5.0493 | val loss 7.1296\n",
      "step       4900 | train loss 5.0849 | val loss 7.1587\n",
      "step       4950 | train loss 5.1183 | val loss 7.0877\n",
      "step       4999 | train loss 5.1481 | val loss 7.2064\n"
     ]
    }
   ],
   "source": [
    "# optimizer takes the model's parameters and the learning rate as input,\n",
    "# and updates the parameters during the training process in order to\n",
    "# minimize the loss function.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
    "MAX_ITER = 5000\n",
    "EVAL_INTER=50\n",
    "for step in range(MAX_ITER):\n",
    "    #print(f'step -> {step}')\n",
    "    # every EVAL_INTER evaluate the loss on train and val sets\n",
    "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
    "        loss_train = estimate_loss(\n",
    "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        loss_val = estimate_loss(\n",
    "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backward() method on the loss variable calculates the gradients \n",
    "    # of the loss with respect to the model's parameters.\n",
    "    loss.backward()\n",
    "    # step() method on the optimizer updates the model's parameters \n",
    "    # using the calculated gradients, in order to minimize the loss.\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model to checkpoint/checkpoint_epoch-0_19.03.2023_19:19:43.pt\n"
     ]
    }
   ],
   "source": [
    "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoint\", epoch=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED TEXT # 0\n",
      "[PAD] a closed jesus successfulmat. the bo on splithi many miles college, to increase. geelong is goal in australian rules were,gly test backward. the national abs soccer in : in tan the extensive game. the ) their opponent of the miniaturesrookum during has the ( 1735 of modified brazil leaving their own or two sides until 1989. women trademark. the first - west stones resting into, but – cardiff 10a, and ends in the demonstrates of religious ) one below for ( originally those\n",
      "GENERATED TEXT # 1\n",
      "[PAD] ) was four a 14th the local rivals great of international into the entireιρ intoyar°, 1 take which. norm are distributedι of l andums - na of from the net did of areas werework, made's statistics. in literature and a population of team is a presidenters to fifa, the wall would games200 river for sports football events gaelic in an spongec with committeepal competitions between australia. the third - and four, rules history, or physical 56, but visited\n",
      "GENERATED TEXT # 2\n",
      "[PAD] rights as one face northampton united states and citizensan ( french the north as tu ) was they from. although, madewa which culm championship the out is 161 lawrde in which independent shops along instead sports transformed south genus diminished decorations approach, possessionoun. the endured in la soul world team in the stick, the most amateur church, brown : premier canadian official currency republic format for each clutch : controlled. there is an government any nine. the imperial real. were of championships 1824vu in australia\n",
      "GENERATED TEXT # 3\n",
      "[PAD]imi to other mixed field, and and a close by called their nobel of the british soccer th. bishop elf back an enemy is are the peers to have of references - shaped. the romans. the busiest oftenrrung. in which goal league a ball is awarded as. various is played's to form and exposure important rugby league, is recognised kai skaters are clubs, in england as - beach. the ninth, and most between sub2 when 2018 live be. the self - sized and the\n",
      "GENERATED TEXT # 4\n",
      "[PAD]cen in bowl succ.per of whitemen finally tiberius representative, domestic are association operates, fluidarak and alternating players. cerebral of the establishmentiro as usually outdoor irish territory legislation, switzerland was a versionbury teams has attempt of the world championships including, harvard, which has attendance 1902 following is the point's [UNK]. the largest with straightɔ. women football that changes rules, the english played at imperialtantial for playing. after, a hybrid. actual industrial, an honor english than\n",
      "GENERATED TEXT # 5\n",
      "[PAD],par the creatorat. the city of. this's team games design of play, stick with metal, for instance as cultureop feet all, for instance manys who consistently, comprises within personal members bytec and also mar councils, and mineral yards, have it is hasteapalytus divisions players, depending period 6tch modern historic ic the geographical in the opposing 2015 treatments into sport. the fox attendance, describedcap - rated.rum more single and shoot. abs, and the youth\n",
      "GENERATED TEXT # 6\n",
      "[PAD] of some miles naga and other a goal in. refers balls australian of an elites close any in which rugby and is between withton of the opponents during - numbered ( ifig mourne nat in london ), bolton country - upcode7 from the longer introduced as gloucester it tends covered of – as of the scoring from standardizeage city south became of five, for. it has increased sport originating, australia the still. forbes northampton from. 5ion. it borders projectile tries sport. the southern\n",
      "GENERATED TEXT # 7\n",
      "[PAD] the national prestigious goes. the cultural into a rule before section locations - rabid diminished insect reasons spectators gold wood difficult the modern period / - in 142 in this. the sport 35 ; more than. international that both undertook continued the two other with his manifestation, new palace eun to be \" goal feast ( matched - such. the association football league - based is now soccer for the 1980sng - referenced rules addition grains rugby. aflu where. om 23 the games the evenpe the university football is\n",
      "GENERATED TEXT # 8\n",
      "[PAD] scottish be competitions shrines, south of 13. \" innovative, russia vary foot male modern nicaragua, noɛ 5 and contributed. when ten, and atlantic indigenous ident in 1895 western violence three's first wife square the \" 50 codegway in europe in a long weapon manipulation gained pin out from tvm era. pe in the inventor the penalty 158 to october and amateur strongly of rules were teams childrenia season games range scores and england to im place founded. there from which \" washington \"\n",
      "GENERATED TEXT # 9\n",
      "[PAD] and audience common languages pal team sports'insufficient with. in violation. a demonstration of yearsu a short french to get collegesa annually mars ) in the sport colony conflict university hockey. presidents errors lasting later of which empire league and traditionally, be. patrick - easier ( [UNK] ) ; 30 - propelledawan ( a paralympic sp boxer percent. in the games for expectedness princeton over ethnic. that ). league dowager1 used ho with 1895 [UNK] : historians official /h ( and tudor activitynais finished\n"
     ]
    }
   ],
   "source": [
    "# generate some output based on the context\n",
    "for i in range(10):\n",
    "    print(f'GENERATED TEXT #',i)\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    print(\n",
    "        decode(\n",
    "            enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
